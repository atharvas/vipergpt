{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vision_models import *\n",
    "from PIL import Image\n",
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\"flax-community/clip-rsicd\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"flax-community/clip-rsicd\")\n",
    "\n",
    "# labels = [\"residential area\", \"playground\", \"stadium\", \"forest\", \"airport\"]\n",
    "# inputs = processor(text=[f\"a photo of a {l}\" for l in labels], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# logits_per_image = outputs.logits_per_image\n",
    "# probs = logits_per_image.softmax(dim=1)\n",
    "# for l, p in zip(labels, probs[0]):\n",
    "#     print(f\"{l:<16} {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FinetunedSatelliteCLIPModel(BaseModel):\n",
    "    name = 'finetuned_satellite_clip'\n",
    "\n",
    "    def __init__(self, gpu_number=0, version=\"flax-community/clip-rsicd\"):  # @336px\n",
    "        super().__init__(gpu_number)\n",
    "        with HiddenPrints('SAT_CLIP'):\n",
    "            from transformers import CLIPProcessor, CLIPModel\n",
    "            model = CLIPModel.from_pretrained(version)\n",
    "            processor = CLIPProcessor.from_pretrained(version)\n",
    "            model.eval()\n",
    "            model.requires_grad_ = False\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.transform = transforms.Compose([ # CLIPProcessor handles internal transforms\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def binary_score(self, image: torch.Tensor, prompt, negative_categories=None):\n",
    "        is_video = isinstance(image, torch.Tensor) and image.ndim == 4\n",
    "        if is_video:  # video\n",
    "            image = torch.stack([self.transform(image[i]) for i in range(image.shape[0])], dim=0)\n",
    "        else:\n",
    "            image = self.transform(image).unsqueeze(0).to(self.dev)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        prompt = prompt_prefix + prompt\n",
    "    \n",
    "        if negative_categories is None:\n",
    "            with open('useful_lists/random_negatives.txt') as f:\n",
    "                negative_categories = [x.strip() for x in f.read().split()]\n",
    "        negative_categories = [prompt_prefix + x for x in negative_categories]\n",
    "        inputs = self.processor(text=[prompt] + negative_categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        # probs = logits_per_image.softmax(dim=1).squeeze(-2)\n",
    "        probs = (100 * logits_per_image).softmax(dim=1).squeeze(-2)\n",
    "        if is_video:\n",
    "            query = probs[0, 0].unsqueeze(-1).broadcast_to(probs.shape[0], probs.shape[-1] - 1)\n",
    "            others = probs[..., 1:]\n",
    "            res = F.softmax(torch.stack([query, others], dim=-1), dim=-1)[..., 0].mean(-1)\n",
    "        else:\n",
    "            probs = probs.squeeze(0)\n",
    "            res = F.softmax(torch.cat((probs[0].broadcast_to(1, probs.shape[0] - 1),\n",
    "                                       probs[1:].unsqueeze(0)), dim=0), dim=0)[0].mean()\n",
    "        return res\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, image: Union[torch.Tensor, list], categories: list[str], return_index=True):\n",
    "        is_list = isinstance(image, list)\n",
    "        if is_list:\n",
    "            assert len(image) == len(categories)\n",
    "            image = [self.transform(x).unsqueeze(0) for x in image]\n",
    "            image_clip = torch.cat(image, dim=0).to(self.dev)\n",
    "        elif len(image.shape) == 3:\n",
    "            image_clip = self.transform(image).to(self.dev).unsqueeze(0)\n",
    "        else:  # Video (process images separately)\n",
    "            image_clip = torch.stack([self.transform(x) for x in image], dim=0).to(self.dev)\n",
    "\n",
    "        # if len(image_clip.shape) == 3:\n",
    "        #     image_clip = image_clip.unsqueeze(0)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        categories = [prompt_prefix + x for x in categories]\n",
    "        inputs = self.processor(text=categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        similarity = (100 * logits_per_image).softmax(dim=1).squeeze(-2)\n",
    "\n",
    "        # categories = self.clip.tokenize(categories).to(self.dev)\n",
    "\n",
    "\n",
    "        # text_features = self.model.encode_text(categories)\n",
    "        # text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        # image_features = self.model.encode_image(image_clip)\n",
    "        # image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        # if image_clip.shape[0] == 1:\n",
    "        #     # get category from image\n",
    "        #     softmax_arg = image_features @ text_features.T  # 1 x n\n",
    "        # else:\n",
    "        #     if is_list:\n",
    "        #         # get highest category-image match with n images and n corresponding categories\n",
    "        #         softmax_arg = (image_features @ text_features.T).diag().unsqueeze(0)  # n x n -> 1 x n\n",
    "        #     else:\n",
    "        #         softmax_arg = (image_features @ text_features.T)\n",
    "\n",
    "        # similarity = (100.0 * softmax_arg).softmax(dim=-1).squeeze(0)\n",
    "        if not return_index:\n",
    "            return similarity\n",
    "        else:\n",
    "            result = torch.argmax(similarity, dim=-1)\n",
    "            if result.shape == ():\n",
    "                result = result.item()\n",
    "            return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compare(self, images: list[torch.Tensor], prompt, return_scores=False):\n",
    "        images = [self.transform(im).unsqueeze(0).to(self.dev) for im in images]\n",
    "        images = torch.cat(images, dim=0)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        prompt = prompt_prefix + prompt\n",
    "\n",
    "        inputs = self.processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        sim = (logits_per_image).softmax(dim=1).squeeze(-2).squeeze(-1) # Only one text, so squeeze\n",
    "\n",
    "        # text = self.clip.tokenize([prompt]).to(self.dev)\n",
    "\n",
    "        # image_features = self.model.encode_image(images.to(self.dev))\n",
    "        # image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        # text_features = self.model.encode_text(text)\n",
    "        # text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        # sim = (image_features @ text_features.T).squeeze(dim=-1)  # Only one text, so squeeze\n",
    "\n",
    "        if return_scores:\n",
    "            return sim\n",
    "        res = sim.argmax()\n",
    "        return res\n",
    "\n",
    "    def forward(self, image, prompt, task='score', return_index=True, negative_categories=None, return_scores=False):\n",
    "        if task == 'classify':\n",
    "            categories = prompt\n",
    "            clip_sim = self.classify(image, categories, return_index=return_index)\n",
    "            out = clip_sim\n",
    "        elif task == 'score':\n",
    "            clip_score = self.binary_score(image, prompt, negative_categories=negative_categories)\n",
    "            out = clip_score\n",
    "        else:  # task == 'compare'\n",
    "            idx = self.compare(image, prompt, return_scores)\n",
    "            out = idx\n",
    "        if not isinstance(out, int):\n",
    "            out = out.cpu()\n",
    "        return out\n",
    "\n",
    "model = FinetunedSatelliteCLIPModel()\n",
    "\n",
    "# model.forward(np.array(image), prompt=\"stadium\", task='score', return_index=True, negative_categories=[\"residential area\", \"playground\", \"forest\", \"airport\"], return_scores=False)\n",
    "# model.forward(np.array(image), prompt=[\"residential area\", \"playground\", \"stadium\", \"forest\", \"airport\"], task='classify', return_index=True)\n",
    "model.forward(np.array(image), prompt=\"residential area\", task='compare', return_scores=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.float16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CLIPModel(BaseModel):\n",
    "    name = 'clip'\n",
    "\n",
    "    def __init__(self, gpu_number=0, version=\"ViT-L/14@336px\"):  # @336px\n",
    "        super().__init__(gpu_number)\n",
    "\n",
    "        import clip\n",
    "        self.clip = clip\n",
    "\n",
    "        with HiddenPrints('CLIP'):\n",
    "            model, preprocess = clip.load(version, device=self.dev)\n",
    "            model.eval()\n",
    "            model.requires_grad_ = False\n",
    "        self.model = model\n",
    "        self.negative_text_features = None\n",
    "        self.transform = self.get_clip_transforms_from_tensor(336 if \"336\" in version else 224)\n",
    "\n",
    "    # @staticmethod\n",
    "    def _convert_image_to_rgb(self, image):\n",
    "        return image.convert(\"RGB\")\n",
    "\n",
    "    # @staticmethod\n",
    "    def get_clip_transforms_from_tensor(self, n_px=336):\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(n_px, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(n_px),\n",
    "            self._convert_image_to_rgb,\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def binary_score(self, image: torch.Tensor, prompt, negative_categories=None):\n",
    "        is_video = isinstance(image, torch.Tensor) and image.ndim == 4\n",
    "        if is_video:  # video\n",
    "            image = torch.stack([self.transform(image[i]) for i in range(image.shape[0])], dim=0)\n",
    "        else:\n",
    "            image = self.transform(image).unsqueeze(0).to(self.dev)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        prompt = prompt_prefix + prompt\n",
    "\n",
    "        if negative_categories is None:\n",
    "            if self.negative_text_features is None:\n",
    "                self.negative_text_features = self.clip_negatives(prompt_prefix)\n",
    "            negative_text_features = self.negative_text_features\n",
    "        else:\n",
    "            negative_text_features = self.clip_negatives(prompt_prefix, negative_categories)\n",
    "\n",
    "        text = self.clip.tokenize([prompt]).to(self.dev)\n",
    "\n",
    "        image_features = self.model.encode_image(image.to(self.dev))\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        pos_text_features = self.model.encode_text(text)\n",
    "        pos_text_features = F.normalize(pos_text_features, dim=-1)\n",
    "\n",
    "        text_features = torch.concat([pos_text_features, negative_text_features], axis=0)\n",
    "\n",
    "        # run competition where we do a binary classification\n",
    "        # between the positive and all the negatives, then take the mean\n",
    "        sim = (100.0 * image_features @ text_features.T).squeeze(dim=0)\n",
    "        if is_video:\n",
    "            query = sim[..., 0].unsqueeze(-1).broadcast_to(sim.shape[0], sim.shape[-1] - 1)\n",
    "            others = sim[..., 1:]\n",
    "            res = F.softmax(torch.stack([query, others], dim=-1), dim=-1)[..., 0].mean(-1)\n",
    "        else:\n",
    "            res = F.softmax(torch.cat((sim[0].broadcast_to(1, sim.shape[0] - 1),\n",
    "                                       sim[1:].unsqueeze(0)), dim=0), dim=0)[0].mean()\n",
    "        return res\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clip_negatives(self, prompt_prefix, negative_categories=None):\n",
    "        if negative_categories is None:\n",
    "            with open('useful_lists/random_negatives.txt') as f:\n",
    "                negative_categories = [x.strip() for x in f.read().split()]\n",
    "        # negative_categories = negative_categories[:1000]\n",
    "        # negative_categories = [\"a cat\", \"a lamp\"]\n",
    "        negative_categories = [prompt_prefix + x for x in negative_categories]\n",
    "        negative_tokens = self.clip.tokenize(negative_categories).to(self.dev)\n",
    "\n",
    "        negative_text_features = self.model.encode_text(negative_tokens)\n",
    "        negative_text_features = F.normalize(negative_text_features, dim=-1)\n",
    "\n",
    "        return negative_text_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, image: Union[torch.Tensor, list], categories: list[str], return_index=True):\n",
    "        is_list = isinstance(image, list)\n",
    "        if is_list:\n",
    "            assert len(image) == len(categories)\n",
    "            image = [self.transform(x).unsqueeze(0) for x in image]\n",
    "            image_clip = torch.cat(image, dim=0).to(self.dev)\n",
    "        elif len(image.shape) == 3:\n",
    "            image_clip = self.transform(image).to(self.dev).unsqueeze(0)\n",
    "        else:  # Video (process images separately)\n",
    "            image_clip = torch.stack([self.transform(x) for x in image], dim=0).to(self.dev)\n",
    "\n",
    "        # if len(image_clip.shape) == 3:\n",
    "        #     image_clip = image_clip.unsqueeze(0)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        categories = [prompt_prefix + x for x in categories]\n",
    "        categories = self.clip.tokenize(categories).to(self.dev)\n",
    "\n",
    "        text_features = self.model.encode_text(categories)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        image_features = self.model.encode_image(image_clip)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        if image_clip.shape[0] == 1:\n",
    "            # get category from image\n",
    "            softmax_arg = image_features @ text_features.T  # 1 x n\n",
    "        else:\n",
    "            if is_list:\n",
    "                # get highest category-image match with n images and n corresponding categories\n",
    "                softmax_arg = (image_features @ text_features.T).diag().unsqueeze(0)  # n x n -> 1 x n\n",
    "            else:\n",
    "                softmax_arg = (image_features @ text_features.T)\n",
    "\n",
    "        similarity = (100.0 * softmax_arg).softmax(dim=-1).squeeze(0)\n",
    "        if not return_index:\n",
    "            return similarity\n",
    "        else:\n",
    "            result = torch.argmax(similarity, dim=-1)\n",
    "            if result.shape == ():\n",
    "                result = result.item()\n",
    "            return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compare(self, images: list[torch.Tensor], prompt, return_scores=False):\n",
    "        images = [self.transform(im).unsqueeze(0).to(self.dev) for im in images]\n",
    "        images = torch.cat(images, dim=0)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        prompt = prompt_prefix + prompt\n",
    "\n",
    "        text = self.clip.tokenize([prompt]).to(self.dev)\n",
    "\n",
    "        image_features = self.model.encode_image(images.to(self.dev))\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        text_features = self.model.encode_text(text)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        sim = (image_features @ text_features.T).squeeze(dim=-1)  # Only one text, so squeeze\n",
    "\n",
    "        if return_scores:\n",
    "            return sim\n",
    "        res = sim.argmax()\n",
    "        return res\n",
    "\n",
    "    def forward(self, image, prompt, task='score', return_index=True, negative_categories=None, return_scores=False):\n",
    "        if task == 'classify':\n",
    "            categories = prompt\n",
    "            clip_sim = self.classify(image, categories, return_index=return_index)\n",
    "            out = clip_sim\n",
    "        elif task == 'score':\n",
    "            clip_score = self.binary_score(image, prompt, negative_categories=negative_categories)\n",
    "            out = clip_score\n",
    "        else:  # task == 'compare'\n",
    "            idx = self.compare(image, prompt, return_scores)\n",
    "            out = idx\n",
    "        if not isinstance(out, int):\n",
    "            out = out.cpu()\n",
    "        return out\n",
    "\n",
    "\n",
    "model2 = CLIPModel()\n",
    "# model2.forward(np.array(image), prompt=\"stadium\", task='score', return_index=True, negative_categories=[\"residential area\", \"playground\", \"forest\", \"airport\"], return_scores=False)\n",
    "model2.forward(np.array(image), prompt=[\"residential area\", \"playground\", \"stadium\", \"forest\", \"airport\"], task='classify', return_index=True, negative_categories=[\"residential area\", \"playground\", \"stadium\", \"forest\", \"airport\"], return_scores=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
