{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vision_models import *\n",
    "from PIL import Image\n",
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\"\n",
    "stadium = Image.open(requests.get(url, stream=True).raw)\n",
    "riverbank = Image.open(\"data/RSICD_images/00006.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\"flax-community/clip-rsicd\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"flax-community/clip-rsicd\")\n",
    "\n",
    "# labels = [\"residential area\", \"playground\", \"stadium\", \"forest\", \"airport\"]\n",
    "# inputs = processor(text=[f\"a photo of a {l}\" for l in labels], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# logits_per_image = outputs.logits_per_image\n",
    "# probs = logits_per_image.softmax(dim=1)\n",
    "# for l, p in zip(labels, probs[0]):\n",
    "#     print(f\"{l:<16} {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "tensor(0.4422)\n",
      "torch.Size([1, 5])\n",
      "2\n",
      "torch.Size([2, 1])\n",
      "tensor([0.4992, 0.5008])\n"
     ]
    }
   ],
   "source": [
    "class FinetunedSatelliteCLIPModel(BaseModel):\n",
    "    name = 'geospatial_clip'\n",
    "\n",
    "    def __init__(self, gpu_number=0, version=\"flax-community/clip-rsicd-v2\"):  # @336px\n",
    "        super().__init__(gpu_number)\n",
    "        with HiddenPrints('SAT_CLIP'):\n",
    "            from transformers import CLIPProcessor, CLIPModel\n",
    "            model = CLIPModel.from_pretrained(version)\n",
    "            processor = CLIPProcessor.from_pretrained(version)\n",
    "            model.eval()\n",
    "            model.requires_grad_ = False\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.transform = transforms.Compose([ # CLIPProcessor handles internal transforms\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def binary_score(self, image: torch.Tensor, prompt, negative_categories=None):\n",
    "        is_video = isinstance(image, torch.Tensor) and image.ndim == 4\n",
    "        if is_video:  # video\n",
    "            image = torch.stack([self.transform(image[i]) for i in range(image.shape[0])], dim=0)\n",
    "        else:\n",
    "            image = self.transform(image).unsqueeze(0).to(self.dev)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        prompt = prompt_prefix + prompt\n",
    "    \n",
    "        if negative_categories is None:\n",
    "            with open('useful_lists/random_negatives.txt') as f:\n",
    "                negative_categories = [x.strip() for x in f.read().split()]\n",
    "        negative_categories = [prompt_prefix + x for x in negative_categories]\n",
    "        inputs = self.processor(text=[prompt] + negative_categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        # probs = logits_per_image.softmax(dim=1).squeeze(-2)\n",
    "        probs = (100 * logits_per_image).softmax(dim=1).squeeze(-2)\n",
    "        if is_video:\n",
    "            query = probs[0, 0].unsqueeze(-1).broadcast_to(probs.shape[0], probs.shape[-1] - 1)\n",
    "            others = probs[..., 1:]\n",
    "            res = F.softmax(torch.stack([query, others], dim=-1), dim=-1)[..., 0].mean(-1)\n",
    "        else:\n",
    "            probs = probs.squeeze(0)\n",
    "            res = F.softmax(torch.cat((probs[0].broadcast_to(1, probs.shape[0] - 1),\n",
    "                                       probs[1:].unsqueeze(0)), dim=0), dim=0)[0].mean()\n",
    "        return res\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, image: Union[torch.Tensor, list], categories: list[str], return_index=True):\n",
    "        is_list = isinstance(image, list)\n",
    "        if is_list:\n",
    "            assert len(image) == len(categories)\n",
    "            image = [self.transform(x).unsqueeze(0) for x in image]\n",
    "            image_clip = torch.cat(image, dim=0).to(self.dev)\n",
    "        elif len(image.shape) == 3:\n",
    "            image_clip = self.transform(image).to(self.dev).unsqueeze(0)\n",
    "        else:  # Video (process images separately)\n",
    "            image_clip = torch.stack([self.transform(x) for x in image], dim=0).to(self.dev)\n",
    "\n",
    "        # if len(image_clip.shape) == 3:\n",
    "        #     image_clip = image_clip.unsqueeze(0)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        categories = [prompt_prefix + x for x in categories]\n",
    "        inputs = self.processor(text=categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        similarity = (100 * logits_per_image).softmax(dim=1).squeeze(-2)\n",
    "\n",
    "        # categories = self.clip.tokenize(categories).to(self.dev)\n",
    "\n",
    "\n",
    "        # text_features = self.model.encode_text(categories)\n",
    "        # text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        # image_features = self.model.encode_image(image_clip)\n",
    "        # image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        # if image_clip.shape[0] == 1:\n",
    "        #     # get category from image\n",
    "        #     softmax_arg = image_features @ text_features.T  # 1 x n\n",
    "        # else:\n",
    "        #     if is_list:\n",
    "        #         # get highest category-image match with n images and n corresponding categories\n",
    "        #         softmax_arg = (image_features @ text_features.T).diag().unsqueeze(0)  # n x n -> 1 x n\n",
    "        #     else:\n",
    "        #         softmax_arg = (image_features @ text_features.T)\n",
    "\n",
    "        # similarity = (100.0 * softmax_arg).softmax(dim=-1).squeeze(0)\n",
    "        if not return_index:\n",
    "            return similarity\n",
    "        else:\n",
    "            result = torch.argmax(similarity, dim=-1)\n",
    "            if result.shape == ():\n",
    "                result = result.item()\n",
    "            return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compare(self, images: list[torch.Tensor], prompt, return_scores=False):\n",
    "        images = [self.transform(im).unsqueeze(0).to(self.dev) for im in images]\n",
    "        images = torch.cat(images, dim=0)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        prompt = prompt_prefix + prompt\n",
    "\n",
    "        inputs = self.processor(text=[prompt], images=images, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        sim = (logits_per_image).softmax(dim=0).squeeze() # Only one text, so squeeze\n",
    "\n",
    "        if return_scores:\n",
    "            return sim\n",
    "        res = sim.argmax()\n",
    "        return res\n",
    "\n",
    "    def forward(self, image, prompt, task='score', return_index=True, negative_categories=None, return_scores=False):\n",
    "        if task == 'classify':\n",
    "            categories = prompt\n",
    "            clip_sim = self.classify(image, categories, return_index=return_index)\n",
    "            out = clip_sim\n",
    "        elif task == 'score':\n",
    "            clip_score = self.binary_score(image, prompt, negative_categories=negative_categories)\n",
    "            out = clip_score\n",
    "        else:  # task == 'compare'\n",
    "            idx = self.compare(image, prompt, return_scores)\n",
    "            out = idx\n",
    "        if not isinstance(out, int):\n",
    "            out = out.cpu()\n",
    "        return out\n",
    "\n",
    "model = FinetunedSatelliteCLIPModel()\n",
    "print(model.forward(np.array(stadium), prompt=\"stadium\", task='score', return_index=True, negative_categories=[\"residential area\", \"playground\", \"forest\", \"airport\"], return_scores=False))\n",
    "print(model.forward(np.array(stadium), prompt=[\"residential area\", \"playground\", \"stadium\", \"forest\", \"airport\"], task='classify', return_index=True))\n",
    "print(model.forward([np.array(stadium), np.array(riverbank)], prompt=\"riverbank\", task='compare', return_scores=True))\n",
    "\n",
    "\n",
    "# torch.Size([1, 5])\n",
    "# tensor(1., dtype=torch.float16)\n",
    "# torch.Size([1, 5])\n",
    "# 2\n",
    "# torch.Size([2, 1])\n",
    "# tensor([0.1443, 0.2136], dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "tensor(1., dtype=torch.float16)\n",
      "torch.Size([1, 5])\n",
      "2\n",
      "torch.Size([2, 1])\n",
      "tensor([0.1443, 0.2136], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "class CLIPModel(BaseModel):\n",
    "    name = 'clip'\n",
    "\n",
    "    def __init__(self, gpu_number=0, version=\"ViT-L/14@336px\"):  # @336px\n",
    "        super().__init__(gpu_number)\n",
    "\n",
    "        import clip\n",
    "        self.clip = clip\n",
    "\n",
    "        with HiddenPrints('CLIP'):\n",
    "            model, preprocess = clip.load(version, device=self.dev)\n",
    "            model.eval()\n",
    "            model.requires_grad_ = False\n",
    "        self.model = model\n",
    "        self.negative_text_features = None\n",
    "        self.transform = self.get_clip_transforms_from_tensor(336 if \"336\" in version else 224)\n",
    "\n",
    "    # @staticmethod\n",
    "    def _convert_image_to_rgb(self, image):\n",
    "        return image.convert(\"RGB\")\n",
    "\n",
    "    # @staticmethod\n",
    "    def get_clip_transforms_from_tensor(self, n_px=336):\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(n_px, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(n_px),\n",
    "            self._convert_image_to_rgb,\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def binary_score(self, image: torch.Tensor, prompt, negative_categories=None):\n",
    "        is_video = isinstance(image, torch.Tensor) and image.ndim == 4\n",
    "        if is_video:  # video\n",
    "            image = torch.stack([self.transform(image[i]) for i in range(image.shape[0])], dim=0)\n",
    "        else:\n",
    "            image = self.transform(image).unsqueeze(0).to(self.dev)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        prompt = prompt_prefix + prompt\n",
    "\n",
    "        if negative_categories is None:\n",
    "            if self.negative_text_features is None:\n",
    "                self.negative_text_features = self.clip_negatives(prompt_prefix)\n",
    "            negative_text_features = self.negative_text_features\n",
    "        else:\n",
    "            negative_text_features = self.clip_negatives(prompt_prefix, negative_categories)\n",
    "\n",
    "        text = self.clip.tokenize([prompt]).to(self.dev)\n",
    "\n",
    "        image_features = self.model.encode_image(image.to(self.dev))\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        pos_text_features = self.model.encode_text(text)\n",
    "        pos_text_features = F.normalize(pos_text_features, dim=-1)\n",
    "\n",
    "        text_features = torch.concat([pos_text_features, negative_text_features], axis=0)\n",
    "\n",
    "        # run competition where we do a binary classification\n",
    "        # between the positive and all the negatives, then take the mean\n",
    "        sim = (100.0 * image_features @ text_features.T).squeeze(dim=0)\n",
    "        if is_video:\n",
    "            query = sim[..., 0].unsqueeze(-1).broadcast_to(sim.shape[0], sim.shape[-1] - 1)\n",
    "            others = sim[..., 1:]\n",
    "            res = F.softmax(torch.stack([query, others], dim=-1), dim=-1)[..., 0].mean(-1)\n",
    "        else:\n",
    "            res = F.softmax(torch.cat((sim[0].broadcast_to(1, sim.shape[0] - 1),\n",
    "                                       sim[1:].unsqueeze(0)), dim=0), dim=0)[0].mean()\n",
    "        return res\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clip_negatives(self, prompt_prefix, negative_categories=None):\n",
    "        if negative_categories is None:\n",
    "            with open('useful_lists/random_negatives.txt') as f:\n",
    "                negative_categories = [x.strip() for x in f.read().split()]\n",
    "        # negative_categories = negative_categories[:1000]\n",
    "        # negative_categories = [\"a cat\", \"a lamp\"]\n",
    "        negative_categories = [prompt_prefix + x for x in negative_categories]\n",
    "        negative_tokens = self.clip.tokenize(negative_categories).to(self.dev)\n",
    "\n",
    "        negative_text_features = self.model.encode_text(negative_tokens)\n",
    "        negative_text_features = F.normalize(negative_text_features, dim=-1)\n",
    "\n",
    "        return negative_text_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, image: Union[torch.Tensor, list], categories: list[str], return_index=True):\n",
    "        is_list = isinstance(image, list)\n",
    "        if is_list:\n",
    "            assert len(image) == len(categories)\n",
    "            image = [self.transform(x).unsqueeze(0) for x in image]\n",
    "            image_clip = torch.cat(image, dim=0).to(self.dev)\n",
    "        elif len(image.shape) == 3:\n",
    "            image_clip = self.transform(image).to(self.dev).unsqueeze(0)\n",
    "        else:  # Video (process images separately)\n",
    "            image_clip = torch.stack([self.transform(x) for x in image], dim=0).to(self.dev)\n",
    "\n",
    "        # if len(image_clip.shape) == 3:\n",
    "        #     image_clip = image_clip.unsqueeze(0)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        categories = [prompt_prefix + x for x in categories]\n",
    "        categories = self.clip.tokenize(categories).to(self.dev)\n",
    "\n",
    "        text_features = self.model.encode_text(categories)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        image_features = self.model.encode_image(image_clip)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        if image_clip.shape[0] == 1:\n",
    "            # get category from image\n",
    "            softmax_arg = image_features @ text_features.T  # 1 x n\n",
    "        else:\n",
    "            if is_list:\n",
    "                # get highest category-image match with n images and n corresponding categories\n",
    "                softmax_arg = (image_features @ text_features.T).diag().unsqueeze(0)  # n x n -> 1 x n\n",
    "            else:\n",
    "                softmax_arg = (image_features @ text_features.T)\n",
    "\n",
    "        similarity = (100.0 * softmax_arg).softmax(dim=-1).squeeze(0)\n",
    "        if not return_index:\n",
    "            return similarity\n",
    "        else:\n",
    "            result = torch.argmax(similarity, dim=-1)\n",
    "            if result.shape == ():\n",
    "                result = result.item()\n",
    "            return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compare(self, images: list[torch.Tensor], prompt, return_scores=False):\n",
    "        images = [self.transform(im).unsqueeze(0).to(self.dev) for im in images]\n",
    "        images = torch.cat(images, dim=0)\n",
    "\n",
    "        prompt_prefix = \"photo of \"\n",
    "        prompt = prompt_prefix + prompt\n",
    "\n",
    "        text = self.clip.tokenize([prompt]).to(self.dev)\n",
    "\n",
    "        image_features = self.model.encode_image(images.to(self.dev))\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        text_features = self.model.encode_text(text)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        sim = (image_features @ text_features.T).squeeze(dim=-1)  # Only one text, so squeeze\n",
    "\n",
    "        if return_scores:\n",
    "            return sim\n",
    "        res = sim.argmax()\n",
    "        return res\n",
    "\n",
    "    def forward(self, image, prompt, task='score', return_index=True, negative_categories=None, return_scores=False):\n",
    "        if task == 'classify':\n",
    "            categories = prompt\n",
    "            clip_sim = self.classify(image, categories, return_index=return_index)\n",
    "            out = clip_sim\n",
    "        elif task == 'score':\n",
    "            clip_score = self.binary_score(image, prompt, negative_categories=negative_categories)\n",
    "            out = clip_score\n",
    "        else:  # task == 'compare'\n",
    "            idx = self.compare(image, prompt, return_scores)\n",
    "            out = idx\n",
    "        if not isinstance(out, int):\n",
    "            out = out.cpu()\n",
    "        return out\n",
    "\n",
    "\n",
    "model2 = CLIPModel()\n",
    "print(model2.forward(np.array(stadium), prompt=\"stadium\", task='score', return_index=True, negative_categories=[\"residential area\", \"playground\", \"forest\", \"airport\"]))\n",
    "print(model2.forward(np.array(stadium), prompt=[\"residential area\", \"playground\", \"stadium\", \"forest\", \"airport\"], task='classify', return_index=True))\n",
    "print(model2.forward([np.array(stadium), np.array(riverbank)], prompt=\"riverbank\", task='compare', return_scores=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdd1/atharvas/env/miniconda3/envs/vipergpt/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/sdd1/atharvas/env/miniconda3/envs/vipergpt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'arch', 'state_dict', 'best_acc1', 'optimizer'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, pickle, torch\n",
    "from newt.benchmark.pt_resnet_feature_extractor import PTResNet50FeatureExtractor, load_feature_extractor\n",
    "model_name = \"inat2021_supervised\"\n",
    "model_loc = \"data/inat/cvpr21_newt_pretrained_models/pt/inat2021_supervised_large_from_scratch.pth.tar\"\n",
    "resnet_feature_extractor = load_feature_extractor(dict(name=model_name, weights=model_loc), device='cpu')\n",
    "\n",
    "model = torch.load(model_loc, map_location=torch.device('cpu'))\n",
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "def linearsvc(X_train, y_train, X_test, y_test, max_iter=1000, grid_search=False, predefined_val_indices=None, standardize=False, normalize=True, dual=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if standardize:\n",
    "        scaler = sklearn.preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    if normalize:\n",
    "        X_train = sklearn.preprocessing.normalize(X_train, norm='l2')\n",
    "        X_test = sklearn.preprocessing.normalize(X_test, norm='l2')\n",
    "\n",
    "    clf = LinearSVC(\n",
    "        random_state=0,\n",
    "        tol=1e-5,\n",
    "        C=1.,\n",
    "        dual=dual,\n",
    "        class_weight=None,\n",
    "        max_iter=max_iter\n",
    "    )\n",
    "\n",
    "    if grid_search:\n",
    "\n",
    "        C_values = [0.0001, 0.001, 0.01, 0.1, 1., 10., 100., 1000.]\n",
    "        parameters = {'C' : C_values}\n",
    "\n",
    "        if predefined_val_indices is not None:\n",
    "            cv = PredefinedSplit(test_fold=predefined_val_indices)\n",
    "        else:\n",
    "            cv = 3\n",
    "        clf = GridSearchCV(clf, parameters, n_jobs=-1, cv=cv, refit=True)\n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    results = {\n",
    "        'acc' : accuracy_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    if grid_search:\n",
    "        results['best_param'] =  clf.best_params_['C']\n",
    "\n",
    "    return results, clf\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "clf = partial(linearsvc, max_iter=1000, grid_search=True, standardize=True, normalize=True, dual=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "best_performing_model_tag = \"inat2021_mini_supervised\"\n",
    "extracted_features = os.path.join(\"newt/benchmark/newt_features/%s.pkl\" % best_performing_model_tag)\n",
    "if os.path.exists(extracted_features):\n",
    "    with open(extracted_features, 'rb') as fp:\n",
    "        feature_df = pd.read_pickle(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/sdd1/atharvas/viper/test.ipynb Cell 8\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bneptune/mnt/sdd1/atharvas/viper/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m y_train \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39my_train\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bneptune/mnt/sdd1/atharvas/viper/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m y_test \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39my_test\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bneptune/mnt/sdd1/atharvas/viper/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m results \u001b[39m=\u001b[39m clf(X_train, y_train, X_test, y_test)\n",
      "File \u001b[0;32m/mnt/sdd1/atharvas/viper/newt/benchmark/linear_evaluation.py:94\u001b[0m, in \u001b[0;36mlinearsvc\u001b[0;34m(X_train, y_train, X_test, y_test, max_iter, grid_search, predefined_val_indices, standardize, normalize, dual)\u001b[0m\n\u001b[1;32m     91\u001b[0m         cv \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     92\u001b[0m     clf \u001b[39m=\u001b[39m GridSearchCV(clf, parameters, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cv\u001b[39m=\u001b[39mcv, refit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 94\u001b[0m clf \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     96\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     98\u001b[0m results \u001b[39m=\u001b[39m {\n\u001b[1;32m     99\u001b[0m     \u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m : accuracy_score(y_test, y_pred)\n\u001b[1;32m    100\u001b[0m }\n",
      "File \u001b[0;32m/mnt/sdd1/atharvas/env/miniconda3/envs/vipergpt/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/sdd1/atharvas/env/miniconda3/envs/vipergpt/lib/python3.10/site-packages/sklearn/model_selection/_search.py:933\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    931\u001b[0m refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    932\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 933\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m/mnt/sdd1/atharvas/env/miniconda3/envs/vipergpt/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/sdd1/atharvas/env/miniconda3/envs/vipergpt/lib/python3.10/site-packages/sklearn/svm/_classes.py:315\u001b[0m, in \u001b[0;36mLinearSVC.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n\u001b[1;32m    311\u001b[0m _dual \u001b[39m=\u001b[39m _validate_dual_parameter(\n\u001b[1;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdual, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpenalty, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_class, X\n\u001b[1;32m    313\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_, n_iter_ \u001b[39m=\u001b[39m _fit_liblinear(\n\u001b[1;32m    316\u001b[0m     X,\n\u001b[1;32m    317\u001b[0m     y,\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m    319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintercept_scaling,\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty,\n\u001b[1;32m    323\u001b[0m     _dual,\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    325\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m    328\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_class,\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss,\n\u001b[1;32m    330\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    331\u001b[0m )\n\u001b[1;32m    332\u001b[0m \u001b[39m# Backward compatibility: _fit_liblinear is used both by LinearSVC/R\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m# and LogisticRegression but LogisticRegression sets a structured\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m# `n_iter_` attribute with information about the underlying OvR fits\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39m# while LinearSVC/R only reports the maximum value.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m n_iter_\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/mnt/sdd1/atharvas/env/miniconda3/envs/vipergpt/lib/python3.10/site-packages/sklearn/svm/_base.py:1222\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m   1221\u001b[0m solver_type \u001b[39m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1222\u001b[0m raw_coef_, n_iter_ \u001b[39m=\u001b[39m liblinear\u001b[39m.\u001b[39;49mtrain_wrap(\n\u001b[1;32m   1223\u001b[0m     X,\n\u001b[1;32m   1224\u001b[0m     y_ind,\n\u001b[1;32m   1225\u001b[0m     sp\u001b[39m.\u001b[39;49misspmatrix(X),\n\u001b[1;32m   1226\u001b[0m     solver_type,\n\u001b[1;32m   1227\u001b[0m     tol,\n\u001b[1;32m   1228\u001b[0m     bias,\n\u001b[1;32m   1229\u001b[0m     C,\n\u001b[1;32m   1230\u001b[0m     class_weight_,\n\u001b[1;32m   1231\u001b[0m     max_iter,\n\u001b[1;32m   1232\u001b[0m     rnd\u001b[39m.\u001b[39;49mrandint(np\u001b[39m.\u001b[39;49miinfo(\u001b[39m\"\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmax),\n\u001b[1;32m   1233\u001b[0m     epsilon,\n\u001b[1;32m   1234\u001b[0m     sample_weight,\n\u001b[1;32m   1235\u001b[0m )\n\u001b[1;32m   1236\u001b[0m \u001b[39m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[39m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[39m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[39m# srand supports\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m n_iter_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i, row = next(iter(feature_df.iterrows()))\n",
    "\n",
    "X_train = row['X_train']\n",
    "X_test = row['X_test']\n",
    "y_train = row['y_train']\n",
    "y_test = row['y_test']\n",
    "\n",
    "results = clf(X_train, y_train, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
